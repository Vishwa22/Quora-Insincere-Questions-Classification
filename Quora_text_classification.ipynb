{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_text_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "g7nR3EVxhjLc",
        "colab_type": "code",
        "outputId": "e7be5b99-b571-4ce2-bbac-44a647861a32",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import time\n",
        "pd.set_option('max_colwidth',400)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"F-score is ill-defined and being set to 0.0 due to no predicted samples.\")\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SZ8K5vhOoTLD",
        "colab_type": "code",
        "colab": {},
        "outputId": "c37dc4a7-7f4c-43c9-c563-d1217b0fab6f"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKxg2Lh90_gQ",
        "colab_type": "text"
      },
      "source": [
        "CuDNN's implementation of GRU and LSTM is much faster than the regular implementation but they do not run deterministically in TensorFlow and Keras. In this competition were speed is essential you can not afford to keep determinism by using the regular implementation of GRU and LSTM.\n",
        "\n",
        "\n",
        "**PyTorch to the rescue!**\n",
        "In PyTorch, CuDNN determinism is a one-liner: torch.backends.cudnn.deterministic = True. This already solves the problem everyone has had so far with Keras. But that's not the only advantage of PyTorch. PyTorch is:\n",
        "\n",
        "1. Significantly faster than Keras and TensorFlow. Again, speed is important in this competition so this is great.\n",
        "2. has a more pythonic API. Inn TensorFlow there are seemingly tens of thousands of ways to do simple things. PyTorch has (in most cases) one obvious way and is by far not as convoluted as TensorFlow.\n",
        "3. Eager execution. There is no such thing as an execution graph in PyTorch. That makes it much easier to try new things and interact with PyTorch in a notebook.\n",
        "\n",
        "Keras solves some of these problems with TensorFlow but it has a high-level API. I think that when doing research, it is often preferable to be able to interact with the model on a low-level. And you will see that the lower level API still doesn't make it complicated to work with PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAhK66DN4gUE",
        "colab_type": "text"
      },
      "source": [
        "**`seed_torch`** sets the seed for numpy and torch to make sure functions with a random component behave deterministically.\n",
        "\n",
        "**`torch.backends.cudnn.deterministic = true`** sets the CuDNN to deterministic mode.\n",
        "\n",
        "This function allows us to run experiments 100% deterministically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bb40886b64534d7a8c0e424d3f2033e984ca9194",
        "id": "PJAoznKAhjLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_torch(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True    ### maintains CuDNN determinism"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2b77f6a1c831c98851143feb25c9903cb1154bf2",
        "_kg_hide-input": true,
        "id": "OXAmnEouhjLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "sub = pd.read_csv('../input/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "28c5b2320f5ef863df3d0b4e4d9175c59bd61ef0",
        "id": "-yB2K57BhjLv",
        "colab_type": "text"
      },
      "source": [
        "### Data overview\n",
        "\n",
        "This is a kernel competition, where we can't use external data. As a result we can use only train and test datasets as well as embeddings which were provided by organizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1558909b0a5c120c1d5ddc5be4f5a952fcb4971e",
        "id": "6ReJOfaahjLw",
        "colab_type": "code",
        "outputId": "9ad258ce-9446-4fcf-aa56-ef1b643a4ec4",
        "colab": {}
      },
      "source": [
        "import os\n",
        "print('Available embeddings:', os.listdir(\"../input/embeddings/\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available embeddings: ['paragram_300_sl999', 'glove.840B.300d', 'wiki-news-300d-1M', 'GoogleNews-vectors-negative300']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yj3KhLxaoTMR",
        "colab_type": "code",
        "colab": {},
        "outputId": "8df01709-1c6d-4656-f8b0-bbafe27c135e"
      },
      "source": [
        "sns.countplot(train['target'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3aad3b4e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0c7299c8895405ef00049595ead1ef89649ba71b",
        "id": "GNjPkHrwhjLz",
        "colab_type": "code",
        "outputId": "189630f1-1561-4159-c0ae-17a12fca3285",
        "colab": {}
      },
      "source": [
        "train[\"target\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1225312\n",
              "1      80810\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gOoktXR2oTMk",
        "colab_type": "code",
        "colab": {},
        "outputId": "e07eca8f-ab1b-444e-ee36-1f7c213cd234"
      },
      "source": [
        "print(len(train.question_text[train['target'] == 0]) / len(train['question_text']) * 100,'percent of sincere')\n",
        "print(len(train.question_text[train['target'] == 1]) / len(train['question_text']) * 100,'percent of insincere')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93.81298224821265 percent of sincere\n",
            "6.187017751787352 percent of insincere\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cdbe2595e31608b72cfbdc8d4bfc75840bfe3a0d",
        "id": "0274eSlshjL3",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation method and Validation techniques\n",
        "\n",
        "**We have a seriuos disbalance - only ~6% of data are positive.**\n",
        "\n",
        "\n",
        "And it is quite a natural trend for these kind of tasks. We will expect to have very few malicious / insincere / obscure questions. Here positive means sort of\n",
        "insincere question which will probably collapse on Quora platform and negative means a normal question. \n",
        "\n",
        "Usually in unbalanced datasets, F1-score is considered as a good metric to go to.\n",
        "\n",
        "**The F1 Score is defined as**  `2` `x` (`precision` `x` `recall`)`/`(`precision` `+` `recall`)\n",
        "\n",
        "It is also called the F Score or the F Measure. \n",
        "Put another way, the F1 score conveys the balance between the precision and the recall. So what is precision and recall??\n",
        "\n",
        "**Precision** =`True Positives` `/` (`True Positives` + `False Positives`). \n",
        "\n",
        "In other words, it is the number of positive predictions divided by the total number of positive class values predicted.\n",
        "\n",
        "**Recall** =`True Positives``/`(`True Positives` `+` `False Negatives`). \n",
        "\n",
        "In other words, it is the number of positive predictions divided by the number of positive class values in the test data.\n",
        "\n",
        "**When we have less positive examples in our dataset we need to use precision and recall because the ability to detect correctly positive samples is our main focus (correct detection of negatives examples is less important to the problem)**.\n",
        "\n",
        "So for our problem, F1-score is a preferable metric.\n",
        "\n",
        "**Cross-validation procedure:**\n",
        "\n",
        "A common practice in datascience competitons is to iterate over various models to find a better performing model. However, it becomes difficult to distinguish whether any improvement in score is coming because we are capturing relationship better or we are just overfitting the data. To find the right answer to this question, we use validation techniques which helps in achieving more generalized relationships.\n",
        "Cross-validation is a statistical method used to estimate the skill of machine learning models. It helps in comparing models to assure lower bias and variance\n",
        "\n",
        "There are various types of cross-validation. One type is the **`K-fold cross-validation`** which we will use here. \n",
        "\n",
        "In this type, the data set is partitioned into **`K`** equal sized sets where one set is used for testing and the rest of the partitions are used for training. This enables us to run **`K`** different runs, where each partition is once used as a testing set. \n",
        "\n",
        "So, higher the **`K`** ,  more accurate the model evaluation is, but the smaller each testing set is.\n",
        "\n",
        "Particularly, since we have imbalanced data here, we will be using **`Stratified K-fold cross validation`**. \n",
        "\n",
        "Here each fold contains approximately the same percentage of samples for each target class as the complete set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "afaa845d44d72b9997ce037ab547ab4010701311",
        "id": "abCM5buyhjL4",
        "colab_type": "code",
        "outputId": "723dc65d-40d6-4477-b286-847f46f12a67",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    qid  ...   target\n",
              "0  00002165364db923c7e6  ...        0\n",
              "1  000032939017120e6e44  ...        0\n",
              "2  0000412ca6e4628ce2cf  ...        0\n",
              "3  000042bf85aa498cd78e  ...        0\n",
              "4  0000455dfa3e01eae3af  ...        0\n",
              "\n",
              "[5 rows x 3 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00002165364db923c7e6</td>\n",
              "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000032939017120e6e44</td>\n",
              "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000412ca6e4628ce2cf</td>\n",
              "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000042bf85aa498cd78e</td>\n",
              "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000455dfa3e01eae3af</td>\n",
              "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_aV0Qxl0L-I",
        "colab_type": "text"
      },
      "source": [
        "### Common pre-processing for text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pzVv1jQ203c",
        "colab_type": "text"
      },
      "source": [
        "In principle, our preprocessing should match the preprocessing that was used before training the pre-trained word embedding. Since most of the embeddings don’t provide vector values for punctuations and other special chars, the first thing you want to do is to get rid of the special characters in your text data. Look at the function *clean_text(x)*. It accomplishes the necessary cleaning task.\n",
        "\n",
        "Why do we want to replace numbers with #s? Because most embeddings have preprocessed their text like this. Check out *clean_numbers(x)*.\n",
        "\n",
        "Contractions are words that we write with an apostrophe. Examples of contractions are words like “ain’t” or “aren’t”. Since we want to standardize our text, it makes sense to expand these contractions. Below we have done this using a contraction mapping and regex functions. Check out  *_get_mispell* and *replace_typical_misspell* functions..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "4f1838e686d67670bb6429b8b43619a69803fde8",
        "id": "wVaHXCOZhjML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    return x\n",
        "\n",
        "def clean_numbers(x):\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n",
        "\n",
        "mispell_dict = {\"aren't\" : \"are not\",\n",
        "\"can't\" : \"cannot\",\n",
        "\"couldn't\" : \"could not\",\n",
        "\"didn't\" : \"did not\",\n",
        "\"doesn't\" : \"does not\",\n",
        "\"don't\" : \"do not\",\n",
        "\"hadn't\" : \"had not\",\n",
        "\"hasn't\" : \"has not\",\n",
        "\"haven't\" : \"have not\",\n",
        "\"he'd\" : \"he would\",\n",
        "\"he'll\" : \"he will\",\n",
        "\"he's\" : \"he is\",\n",
        "\"i'd\" : \"I would\",\n",
        "\"i'd\" : \"I had\",\n",
        "\"i'll\" : \"I will\",\n",
        "\"i'm\" : \"I am\",\n",
        "\"isn't\" : \"is not\",\n",
        "\"it's\" : \"it is\",\n",
        "\"it'll\":\"it will\",\n",
        "\"i've\" : \"I have\",\n",
        "\"let's\" : \"let us\",\n",
        "\"mightn't\" : \"might not\",\n",
        "\"mustn't\" : \"must not\",\n",
        "\"shan't\" : \"shall not\",\n",
        "\"she'd\" : \"she would\",\n",
        "\"she'll\" : \"she will\",\n",
        "\"she's\" : \"she is\",\n",
        "\"shouldn't\" : \"should not\",\n",
        "\"that's\" : \"that is\",\n",
        "\"there's\" : \"there is\",\n",
        "\"they'd\" : \"they would\",\n",
        "\"they'll\" : \"they will\",\n",
        "\"they're\" : \"they are\",\n",
        "\"they've\" : \"they have\",\n",
        "\"we'd\" : \"we would\",\n",
        "\"we're\" : \"we are\",\n",
        "\"weren't\" : \"were not\",\n",
        "\"we've\" : \"we have\",\n",
        "\"what'll\" : \"what will\",\n",
        "\"what're\" : \"what are\",\n",
        "\"what's\" : \"what is\",\n",
        "\"what've\" : \"what have\",\n",
        "\"where's\" : \"where is\",\n",
        "\"who'd\" : \"who would\",\n",
        "\"who'll\" : \"who will\",\n",
        "\"who're\" : \"who are\",\n",
        "\"who's\" : \"who is\",\n",
        "\"who've\" : \"who have\",\n",
        "\"won't\" : \"will not\",\n",
        "\"wouldn't\" : \"would not\",\n",
        "\"you'd\" : \"you would\",\n",
        "\"you'll\" : \"you will\",\n",
        "\"you're\" : \"you are\",\n",
        "\"you've\" : \"you have\",\n",
        "\"'re\": \" are\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'll\":\" will\",\n",
        "\"didn't\": \"did not\",\n",
        "\"tryin'\":\"trying\"}\n",
        "\n",
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re\n",
        "\n",
        "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
        "def replace_typical_misspell(text):\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "    return mispellings_re.sub(replace, text)\n",
        "\n",
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: x.lower())\n",
        "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: x.lower())\n",
        "\n",
        "# Clean the text\n",
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\n",
        "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))\n",
        "\n",
        "# Clean numbers\n",
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
        "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
        "\n",
        "# Clean speelings\n",
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
        "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed0n3cADMu-H",
        "colab_type": "text"
      },
      "source": [
        "### Representation of text string by a list of integer numbers\n",
        "\n",
        "One of the things that have made Deep Learning the go to choice for NLP is the fact that we don’t really have to hand-engineer features from the text data. \n",
        "\n",
        "The deep learning algorithms take as input a sequence of text to learn the structure of text just like a human does. Since Machine cannot understand words they expect their data in numerical form. So we would like to represent out text data as a series of numbers.\n",
        "\n",
        "To understand how this is done we need to understand a little about the **`Keras Tokenizer`** function. One can use any other tokenizer also. Here we will be using Keras `Tokenizer`.\n",
        "\n",
        "The `num_words parameter` keeps a pre-specified number of words in the text only. This is helpful as we don’t want our models to get a lot of noise by considering words that occur very infrequently. In real-world data, most of the words we leave using num_words parameter are normally misspells. The tokenizer also filters some non-wanted tokens by default and converts the text into lowercase.\n",
        "\n",
        "The tokenizer once fitted to the data also keeps an index of words (dictionary of words which we can use to assign a unique number to a word) which can be accessed by `tokenizer.word_index`. The words in the indexed dictionary are ranked in order of frequencies.\n",
        "\n",
        "Normally our model expects that each sequence (each training example) will be of the same length (same number of words/tokens) so that we can process the training samples in batches. We can control this using the `maxlen` parameter in Keras `pad_sequences`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9d081b2f0d46faf01a943c309568c27f92462f94",
        "id": "CSvWTyRShjMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 120000\n",
        "tk = Tokenizer(lower = True, filters='', num_words=max_features)\n",
        "\n",
        "### we will make a list of questions from training data as well as test data together as \"full_text\" and \n",
        "### perform tokenizer fitting on \"full_text\" \n",
        "\n",
        "full_text = list(train['question_text'].values) + list(test['question_text'].values)\n",
        "tk.fit_on_texts(full_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "33929a60e1872b73e40424daa1178a3d8fbf8f5a",
        "id": "Xa849J6_hjMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\n",
        "test_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "4330f6c01064b5fda4ca9661dc4f1cefb439cf75",
        "id": "U8L8R7fFhjMV",
        "colab_type": "code",
        "outputId": "f25f86d4-dc7c-4b43-fa47-a103187a95de",
        "colab": {}
      },
      "source": [
        "train['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\n",
        "plt.yscale('log');\n",
        "plt.title('Distribution of question text length');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "05c7e2c63afb343b64835432721a42f81dd53626",
        "id": "NONUdzPxhjMr",
        "colab_type": "text"
      },
      "source": [
        "How do you know what is the best \"maxlen\" to set? If you put it too short, you might lose some useful feature that could cost you some accuracy points down the path.If you put it too long, your LSTM cell will have to be larger to store the possible values or states.\n",
        "\n",
        "One of the ways to go about it is to see the distribution of the number of words in sentences.\n",
        "\n",
        "We can see that most of the questions are 40 words long or shorter. Let's try having sequence length equal to 72 for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "15d7e3a2fb5fe5fb38da45ca3d0bd13c82b7eda5",
        "id": "At8XQ8TOhjMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 72\n",
        "maxlen = 72\n",
        "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
        "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "31839c56ed1d7945decf176c97b49f0205cc40af",
        "id": "KSQE7Dj7hjMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = train['target'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "69d325ea77439697e53143adef3a0da58e7f31f2",
        "id": "kOKN_2E9hjM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsOwb8yAiE9",
        "colab_type": "text"
      },
      "source": [
        "Let's define **4-Fold cross-validation**. The *random_state* here is important to make sure this is deterministic too.\n",
        "\n",
        "**cross-validation splits training data into \"train\" and \"validation\" sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cca23aa2dc1207a6f08212ad8d87f8182e567e0e",
        "id": "6Kdjfg3ghjM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=10).split(X_train, y_train)) ### Generate indices to split data into training and validation/test set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES47iIvMQB4Y",
        "colab_type": "text"
      },
      "source": [
        "### Word embeddings and the importance of Embedding layer\n",
        "\n",
        "\n",
        "Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors). \n",
        "\n",
        "And most importantly, word embeddings are learned from data. So, word embeddings pack more information into far fewer dimensions. The simplest way to associate a dense vector with a word is to choose the vector at random. In that case, the resulting embedding space will have no structure. \n",
        "\n",
        "For example, “accurate” and “exact” may end up with completely different embeddings, even when from language point of view, they are synonyms. In order that the deep neural network can get some linguistic understanding from our embedding space we need the vectors of similar words to have higher value for cosine similarity. \n",
        "\n",
        "In real-world word-embedding spaces, common examples of meaningful geometric transformations are “gender” vectors and “plural” vectors. That is, it enables us to do algebraic manipulations on words which were not possible before. For example: What is king - man + woman? It comes out to be Queen.\n",
        "\n",
        "Word embeddings vectors also help us to find out the similarity between words. If we try to find similar words to “good”, we will find “awesome”, “great”, etc. It is this property of word embedding vectors that makes it invaluable for text classification. Now our deep learning network understands that “good” and “great” are essentially words with similar meaning.\n",
        "\n",
        "There are two ways to obtain word embeddings:\n",
        " Learn word embeddings jointly with the main classification task. Start with random vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
        " Load into your model word embeddings that were precomputed using a different machine-learning task than the one you’re trying to solve. These are called pre-trained word embeddings.\n",
        "Usually, pre-trained word vectors are provided to us by others after training on large corpora of texts like Wikipedia, twitter etc.\n",
        "This competition provides three different embeddings which can be used for word representations.\n",
        "\n",
        "First one is **Glove.840B.300d.txt**. It will contain embeddings of dimension 300. We need to parse this file to build an index that maps words (as strings) to their vector representation (as number vectors).\n",
        "\n",
        "`embedding_index` is just a dictionary in which the key is the word and the value is the word vector, a np.array of length 300. The length of this dictionary is somewhere around a billion. \n",
        "\n",
        "Since we only want the embeddings of words that are in our `word_index`, we will create a matrix which just contains required embeddings.\n",
        "\n",
        "We will repeat the procedure for two other pre-trained word embeddings available namely\n",
        "**paragram_300_sl999.txt** and  **wiki-news-300d-1M.vec**\n",
        "\n",
        "Later we will combine all the three embedding matrices using *mean* to get a better effect on the training process. Refer to the paper [Meta Embeddings by Averaging Source Word Embeddings](https://www.aclweb.org/anthology/N18-2031)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5cd86c2400db5ea1511b107250c8aa8c98ea909b",
        "id": "St-hiLAyhjM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2948d63cafb76b446c5e167a6dce7915ca43da6d",
        "id": "lgPlUM4ahjNC",
        "colab_type": "code",
        "outputId": "5c11ed76-6e42-4d87-fdf4-24de49d7fec0",
        "colab": {}
      },
      "source": [
        "embedding_path = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n",
        "\n",
        "all_embs = np.stack(embedding_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "\n",
        "word_index = tk.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cc553dd7ddbd3a81ed1a39084fdb4982b5887971",
        "id": "kLcENMBbhjNG",
        "colab_type": "code",
        "outputId": "8159c588-6d59-46c0-cd16-eb5655841328",
        "colab": {}
      },
      "source": [
        "embedding_path = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore') if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embedding_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "\n",
        "word_index = tk.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "embedding_matrix1 = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix1[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "61b5ee9828ec17775cbda83d5b061cca5746e6c5",
        "id": "ciT9kT03hjNL",
        "colab_type": "code",
        "outputId": "90c558f7-c3e5-4816-bdb3-9ccf7b71f0ed",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "#embed_size = all_embs.shape[1]\n",
        "\n",
        "\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "embedding_matrix2 = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix2[i] = embedding_vector\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "333429eaa0c7c057769518c8f1b0fefbb68f1d1d",
        "scrolled": true,
        "id": "XuqAPI_LhjNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.mean([embedding_matrix, embedding_matrix1, embedding_matrix2], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b758618c32ea1b48c4343c718c75c0eb623a50fd",
        "id": "5t5yh3ULhjNR",
        "colab_type": "code",
        "outputId": "65a68edb-374e-4541-a3f7-ee097c6e92bb",
        "colab": {}
      },
      "source": [
        "del embedding_matrix1, embedding_matrix2\n",
        "import gc   ### gc – Garbage Collector. Purpose: Manages memory used by Python objects. \n",
        "gc.collect()  ### The garbage collection can be invoked manually"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11989"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caBzrusQgPUT",
        "colab_type": "text"
      },
      "source": [
        "### Integral parts of text classification *deep learning model*\n",
        "\n",
        "We have already seen the importance of embedding layer. When we use pretrained word embeddings we have the choice to either allow the embedding to be updated during training or only use the resulting embedding vectors as they are.\n",
        "\n",
        "**Now lets focus on other layers like Bidirectional LSTM layer and Attention Layer:**\n",
        "\n",
        "A text data can be considered as a sequence or a temporal series of data points and in order to interpret something of it, the neural network has to see the entire sequence at once. But processing entire text into a single large vector and passing it into a feedforward networks doesn’t seem quite efficient. Biological intelligence processes information incrementally while maintaining an internal model of what it’s processing, built from past information and constantly updated as new information comes in. You will notice that’s how a human being reads the text, a sentence or paragraph. \n",
        "\n",
        "In short we need our network to be made of cells which have memory of some sorts. RNN cells adopt this principle.\n",
        "\n",
        "RNN cells are able to remember previous information using hidden states and connect it to the current task. \n",
        "\n",
        "Long Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for a long period of time.\n",
        "\n",
        "Now there are several kaggle kernels which shows a decent gain in accuracy by using Bidirectional LSTM.\n",
        "\n",
        "**How does Bidirectional LSTM work?**\n",
        "\n",
        "![image](https://i.imgur.com/jaKiP0S.png)\n",
        "\n",
        "Imagine that the LSTM is split between 2 hidden states for each time step. As the sequence of words is being feed into the LSTM in a forward fashion, there's another reverse sequence that is feeding to the different hidden state at the same time. We will notice at the model summary that the output dimension of LSTM layer has doubled to 120 because 60 dimensions are used for forward, and another 60 are used for reverse.\n",
        "\n",
        "The greatest advantage in using Bidirectional LSTM is that when it runs backwards we preserve information from the future and using the two hidden states combined, so any prediction in the middle of the sentence can be done by taking into account information potentially from the entire sequence.\n",
        "\n",
        "**Now, let's take our attention to \"Attention layer\"**.\n",
        "\n",
        "Even though attention mechanism was initially designed in the context of Neural Machine Translation using Seq2Seq Models, it has now become one of the most influential ideas in the Deep Learning community.\n",
        "\n",
        "The idea is not every word in a the input sentence contribute equally to the meaning of the sentence. So if we are trying to judge the meaning or sentiment of sentence, we might do well if we focus on particular words more than the others. Its more like giving separate weightage to individual words of the sentence.\n",
        "\n",
        "You can learn more about the basic intuition behind attention mechanism in the following video:\n",
        "\n",
        "https://www.coursera.org/lecture/nlp-sequence-models/attention-model-intuition-RDXpX\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5iOkMt_bM6P",
        "colab_type": "text"
      },
      "source": [
        "### Let's learn a thing or two about PyTorch:\n",
        "\n",
        "**`torch.utils.data.DataLoader`** is an iterator which provides features like:\n",
        "\n",
        "\n",
        "*   Batching the data\n",
        "*   Shuffling the data\n",
        "*   Load the data in parallel using multiprocessing workers.\n",
        "\n",
        "**`class Network(nn.Module):`**\n",
        "\n",
        "            def __init__():\n",
        "                    super().__init__()\n",
        "                    <All the operations are defined here>\n",
        "            def forward(self, x):\n",
        "                    <model flow is defined here>\n",
        "\n",
        "The class is inherited from **`nn.Module`**.\n",
        "\n",
        "This creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
        "\n",
        "PyTorch networks created with `nn.Module` must have a `forward` method defined. The actual model structure about how the sequence in which the tensor input will go through the model is defined here.\n",
        "It takes in a tensor `x` and passes it through the operations you defined in the `__init__()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "79a20aee9c4287bd8bd2556b717dc75be34ca6b3",
        "id": "oQDLrfcWhjNU",
        "colab_type": "text"
      },
      "source": [
        "### Neural net\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "23b0dcfa9bff61ef4c5aebd85f9521edd8f6009d",
        "id": "PAZmo6NohjNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "        \n",
        "        self.supports_masking = True\n",
        "\n",
        "        self.bias = bias\n",
        "        self.feature_dim = feature_dim\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        \n",
        "        weight = torch.zeros(feature_dim, 1)\n",
        "        nn.init.xavier_uniform_(weight)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "        \n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        feature_dim = self.feature_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = torch.mm(\n",
        "            x.contiguous().view(-1, feature_dim), \n",
        "            self.weight\n",
        "        ).view(-1, step_dim)\n",
        "        \n",
        "        if self.bias:\n",
        "            eij = eij + self.b\n",
        "            \n",
        "        eij = torch.tanh(eij)\n",
        "        a = torch.exp(eij)\n",
        "        \n",
        "        if mask is not None:\n",
        "            a = a * mask\n",
        "\n",
        "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
        "\n",
        "        weighted_input = x * torch.unsqueeze(a, -1)\n",
        "        return torch.sum(weighted_input, 1)\n",
        "    \n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        \n",
        "        hidden_size = 128\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        \n",
        "        self.embedding_dropout = nn.Dropout2d(0.15)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        self.lstm_attention = Attention(hidden_size*2, maxlen)\n",
        "        self.gru_attention = Attention(hidden_size*2, maxlen)\n",
        "        self.emb_attention = Attention(embed_size, maxlen)\n",
        "        \n",
        "       \n",
        "        self.linear = nn.Linear(1324, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.out = nn.Linear(128, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
        "        \n",
        "        h_lstm, _ = self.lstm(h_embedding)\n",
        "        h_gru, _ = self.gru(h_lstm)\n",
        "        \n",
        "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
        "        h_gru_atten = self.gru_attention(h_gru)\n",
        "        emb_atten = self.emb_attention(h_embedding)\n",
        "        \n",
        "        avg_pool = torch.mean(h_gru, 1)\n",
        "        max_pool, _ = torch.max(h_gru, 1)\n",
        "        \n",
        "        \n",
        "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool, emb_atten), 1)\n",
        "        conc = self.relu(self.linear(conc))\n",
        "        conc = self.dropout(conc)\n",
        "        out = self.out(conc)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "efa73786bf93892a5c3f46329122c0c0275e4858",
        "id": "HK-YIRl4hjNY",
        "colab_type": "code",
        "colab": {},
        "outputId": "68bf1108-dfe9-4a07-e8a6-348ad99abab7"
      },
      "source": [
        "embed_size=300\n",
        "m = NeuralNet()\n",
        "m"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (embedding): Embedding(120000, 300)\n",
              "  (embedding_dropout): Dropout2d(p=0.15)\n",
              "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
              "  (gru): GRU(256, 128, batch_first=True, bidirectional=True)\n",
              "  (lstm_attention): Attention()\n",
              "  (gru_attention): Attention()\n",
              "  (emb_attention): Attention()\n",
              "  (linear): Linear(in_features=1324, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.15)\n",
              "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a29c7d10faaf54c58cda8a406cb31fb367816f6c",
        "id": "50GlfopvhjNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, x_train, y_train, x_val, y_val, validate=True):\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "  \n",
        "    ### Dataset wrapping tensors\n",
        "    ### tensors that have the same size of the first dimension.\n",
        "    train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "    valid = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "    \n",
        "    ## Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # define binary cross entropy loss\n",
        "    # note that the model returns logit to take advantage of the log-sum-exp trick \n",
        "    # for numerical stability in the loss\n",
        "    \n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
        "    best_score = -np.inf\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        model.train()   # set train mode of the model. This enables operations which are only applied during training like dropout\n",
        "        avg_loss = 0\n",
        "        \n",
        "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
        "            # Forward pass: compute predicted y by passing x to the model.\n",
        "            y_pred = model(x_batch)\n",
        "         \n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            # Before the backward pass, use the optimizer object to zero all of the\n",
        "            # gradients for the tensors(which are the learnable weights\n",
        "            # of the model) so that they don't get added to the previous batch \n",
        "            optimizer.zero_grad()\n",
        "            # Backward pass: compute gradient of the loss with respect to model parameter\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()   ### updating the parameters/weights for each batch run\n",
        "            avg_loss += loss.item() / len(train_loader)\n",
        "        \n",
        "        ## after 1 epoch has passed for entire training set we will go for evaluation on validation and test sets!\n",
        "        model.eval()   # set evaluation mode of the model. \n",
        "        ## This disabled operations which are only applied during training like dropout\n",
        "        \n",
        "        valid_preds = np.zeros((x_val_fold.size(0)))\n",
        "        \n",
        "        if validate:\n",
        "            avg_val_loss = 0\n",
        "            \n",
        "            \n",
        "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "                ## detach() detaches the output from the computationnal graph. So no gradient will be backproped along this variable.\n",
        "                ## refers to only a given variable on which it’s called.\n",
        "                y_pred = model(x_batch).detach()\n",
        "\n",
        "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
        "                valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
        "                \n",
        "                \n",
        "            search_result = threshold_search(y_val.cpu().numpy(), valid_preds)\n",
        "            ## finding the best threshold for validation data\n",
        "            val_f1, val_threshold = search_result['f1'], search_result['threshold']\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} best_t={:.2f} \\t time={:.2f}s'.format(\n",
        "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_f1, val_threshold, elapsed_time))\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
        "                epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
        "    \n",
        "    ### for a given fold it will compute validation predictions and validation loss\n",
        "    valid_preds = np.zeros((x_val_fold.size(0)))\n",
        "    \n",
        "    avg_val_loss = 0.\n",
        "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "        y_pred = model(x_batch).detach()\n",
        "\n",
        "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
        "        valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
        "\n",
        "    print('Validation loss: ', avg_val_loss)\n",
        "\n",
        "    test_preds = np.zeros((len(test_loader.dataset)))\n",
        "    # predict all samples in the test set batch per batch\n",
        "    for i, (x_batch,) in enumerate(test_loader):\n",
        "        y_pred = model(x_batch).detach()\n",
        "\n",
        "        test_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
        "\n",
        "    \n",
        "\n",
        "    return valid_preds, test_preds   ### predictions on validation data and test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v3SJLIp0oTQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "# always call this before training for deterministic results\n",
        "seed = 1234\n",
        "seed_everything()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2252ff85cae8b5096e7f10ccf64ce10299a18782",
        "id": "herXKFakhjNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### preparing the test dataset and test_loader\n",
        "x_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\n",
        "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
        "batch_size = 512\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TPTCGRBRoTRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def threshold_search(y_true, y_proba):\n",
        "    best_threshold = 0\n",
        "    best_score = 0\n",
        "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
        "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
        "        if score > best_score:\n",
        "            best_threshold = threshold\n",
        "            best_score = score\n",
        "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
        "    return search_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "68aa1d6303e656ca76184d53c44001836c431939",
        "scrolled": true,
        "id": "Wup4fKMqhjNm",
        "colab_type": "code",
        "outputId": "1a053a94-4270-4219-940c-082969a59306",
        "colab": {}
      },
      "source": [
        "train_preds = np.zeros(len(train))\n",
        "test_preds = np.zeros((len(test), len(splits)))\n",
        "\n",
        "n_epochs = 4\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "for i, (train_idx, valid_idx) in enumerate(splits):   \n",
        "    ## split data in train / validation according to the KFold indices\n",
        "    ## also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n",
        "    x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n",
        "    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
        "    x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n",
        "    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
        "    \n",
        "\n",
        "    print(f'Fold {i + 1}')\n",
        "    \n",
        "    seed_everything(seed + i)\n",
        "    model = NeuralNet()   ## model defined\n",
        "    model.cuda() ## everything on the model will run on GPU\n",
        "\n",
        "    valid_preds_fold, test_preds_fold = train_model(model,\n",
        "                                                    x_train_fold, \n",
        "                                                    y_train_fold, \n",
        "                                                    x_val_fold, \n",
        "                                                    y_val_fold, validate=True)\n",
        "\n",
        "    train_preds[valid_idx] = valid_preds_fold\n",
        "    test_preds[:, i] = test_preds_fold   ## for each fold predictions for test data is done\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Epoch 1/4 \t loss=0.1342 \t val_loss=0.1120 \t val_f1=0.6591 best_t=0.19 \t time=114.83s\n",
            "Epoch 2/4 \t loss=0.1204 \t val_loss=0.1016 \t val_f1=0.6742 best_t=0.30 \t time=116.10s\n",
            "Epoch 3/4 \t loss=0.1143 \t val_loss=0.0985 \t val_f1=0.6825 best_t=0.36 \t time=116.70s\n",
            "Epoch 4/4 \t loss=0.1087 \t val_loss=0.0972 \t val_f1=0.6853 best_t=0.32 \t time=116.20s\n",
            "Validation loss:  0.09716170831790727\n",
            "Fold 2\n",
            "Epoch 1/4 \t loss=0.1340 \t val_loss=0.1046 \t val_f1=0.6608 best_t=0.30 \t time=117.50s\n",
            "Epoch 2/4 \t loss=0.1205 \t val_loss=0.0991 \t val_f1=0.6824 best_t=0.34 \t time=115.99s\n",
            "Epoch 3/4 \t loss=0.1142 \t val_loss=0.0979 \t val_f1=0.6841 best_t=0.33 \t time=116.14s\n",
            "Epoch 4/4 \t loss=0.1084 \t val_loss=0.0977 \t val_f1=0.6859 best_t=0.31 \t time=116.12s\n",
            "Validation loss:  0.09769480986678286\n",
            "Fold 3\n",
            "Epoch 1/4 \t loss=0.1348 \t val_loss=0.1048 \t val_f1=0.6626 best_t=0.30 \t time=116.16s\n",
            "Epoch 2/4 \t loss=0.1208 \t val_loss=0.0998 \t val_f1=0.6789 best_t=0.27 \t time=116.45s\n",
            "Epoch 3/4 \t loss=0.1148 \t val_loss=0.0988 \t val_f1=0.6810 best_t=0.41 \t time=116.38s\n",
            "Epoch 4/4 \t loss=0.1086 \t val_loss=0.0973 \t val_f1=0.6846 best_t=0.28 \t time=115.67s\n",
            "Validation loss:  0.09732039500506691\n",
            "Fold 4\n",
            "Epoch 1/4 \t loss=0.1342 \t val_loss=0.1065 \t val_f1=0.6595 best_t=0.32 \t time=116.09s\n",
            "Epoch 2/4 \t loss=0.1200 \t val_loss=0.1009 \t val_f1=0.6768 best_t=0.40 \t time=116.17s\n",
            "Epoch 3/4 \t loss=0.1143 \t val_loss=0.0975 \t val_f1=0.6863 best_t=0.37 \t time=115.86s\n",
            "Epoch 4/4 \t loss=0.1082 \t val_loss=0.1013 \t val_f1=0.6840 best_t=0.27 \t time=115.84s\n",
            "Validation loss:  0.1013270846238144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3499ccfe4d8d7e452a6adf428ea79d5070bb1a43",
        "id": "5W2j1RIrhjNr",
        "colab_type": "text"
      },
      "source": [
        "### Test Predictions for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2c04e298f5c96a9561450874a0238f408984c39e",
        "id": "wFpBMKwThjNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search_result = threshold_search(y_train, train_preds)\n",
        "sub['prediction'] = test_preds.mean(1) > search_result['threshold']   ## mean of all folds for test predictions!!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "reHZikTLoTRl",
        "colab_type": "code",
        "colab": {},
        "outputId": "0bf0b1a2-ee0e-4c65-a907-f395429bf71d"
      },
      "source": [
        "search_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'threshold': 0.3, 'f1': 0.6841741373600517}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2KvpXj1dhjNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## sub.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}